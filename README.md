# Evaluating & Fine-Tuning Multilingual Sentence Embeddings  
**Russianâ€“Turkmen Bitext Retrieval & Semantic Similarity**  
Data Science Seminar â€“ University of Passau  
Author: Jelaleddin Sultanov  

---

## ğŸ“Œ Motivation
- Bureaucracy in Turkmenistan relies heavily on **Turkmen and Russian**.  
- No robust MT models exist for Turkmen due to **minimal resources**.  
- Scanned / unsupervised books and e-libraries contain **parallel text** that can be leveraged.  

---

## ğŸ“ˆ Experiment Tracking
All training and evaluation runs tracked with **Weights & Biases**:

- [LaBSE Fine-tuning Logs](https://wandb.ai/jelal/LaBSE?nw=nwuserjelal)  
- [NLLB MT Fine-tuning Logs](https://wandb.ai/jelal/mt-ru-tk?nw=nwuserjelal)  
- [MiniLM Fine-tuning Logs](https://wandb.ai/jelal/sentence-transformers?nw=nwuserjelal)  
---

## ğŸ“‚ Data Sources
1. **Tatoeba Challenge**: ~160K parallel sentence pairs (noisy).  
2. **Uzbek e-Library**: High-school textbooks in Russian, Turkmen, Uzbek, Tajik, Kazakh.  
3. **Scraped books**: Russian â†” Turkmen aligned via embeddings.  
4. **Custom annotation**: Manual cleaning and labeling via [Label Studio instance](https://ai-ls-app-872f442e1e27.herokuapp.com/user/login/)  
   - **Username:** `jelal@obamedica.com`  
   - **Password:** `jelal99`  

---

## ğŸ“š Custom Dataset Collection

To build a high-quality **Russianâ€“Turkmen parallel corpus**, we designed a semi-automated pipeline (see figure below):

![Data Collection Workflow](DATA%20SCIENCE%20SEMINAR.png)

**Steps:**
1. **Data Sources**  
   - Uzbek high-school e-library (Turkmen & Russian textbooks).  
   - Turkmen news websites and books with multiple translations.  

2. **Text Extraction & Preprocessing**  
   - Scraped Russian and Turkmen text into a raw corpus.  

3. **Embedding-based Alignment**  
   - Generated sentence embeddings of sentences from raw corpus  
   - Matched Russianâ€“Turkmen pairs using **cosine similarity**.  

4. **Human-in-the-Loop Annotation**  
   - Candidate parallel sentences were reviewed in **Label Studio**.  
   - Manual cleaning ensured higher alignment accuracy.  

5. **Final Aligned Dataset**  
   - Produced a curated dataset of **Turkmenâ€“Russian parallel sentences** for training and evaluation.  
---


## ğŸ§  Models Evaluated
- **[LaBSE](https://arxiv.org/abs/2007.01852)** (Google Research)  
  Dual-encoder optimized for cross-lingual retrieval.  
- **[NLLB-200](https://aclanthology.org/2022.acl-long.62/)** (Meta AI)  
  Seq2Seq MT model supporting 200+ languages.  
- **[MiniLM](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)**  
  Lightweight multilingual embedding model.  

Fine-tuning performed with **LoRA adapters** for efficiency.

---

## âš™ï¸ Fine-Tuning Setup
- Framework: [Sentence-Transformers](https://www.sbert.net/)  
- Loss functions: **Multiple Negative Ranking Loss (MNRL)** for bitext, cosine loss for STS.  
- Evaluation metrics:  
  - **Bitext Retrieval**: Precision@1, MRR  
  - **STS (Semantic Textual Similarity)**: Pearson r, Spearman Ï  
  - **MT (NLLB)**: BLEU, chrF, TER  

---

## ğŸ“Š Results Summary
### Bitext Retrieval
- **LaBSE (fine-tuned w/ LoRA)** â†’ P@1 = **1.000**, MRR = **1.000**  
- **NLLB (pretrained)** â†’ P@1 = 1.000, MRR = 1.000  
- **LaBSE (pretrained)** â†’ P@1 = 0.889, MRR = 0.944  
- **MiniLM (baseline)** â†’ P@1 = 0.556, MRR = 0.701  

### Semantic Textual Similarity (STS17 RU)
- **MiniLM** â†’ Pearson 0.7893  
- **LaBSE (pretrained)** â†’ 0.7357  
- **NLLB (pretrained)** â†’ 0.6996  
- **LaBSE (fine-tuned)** â†’ 0.6715  

---

## ğŸš€ Key Insights
- **Dataset quality** is the bottleneck: high-quality, domain-specific parallel corpora are needed for Turkmen NLP.  
- **LaBSE + LoRA** adapters work extremely well for **bitext retrieval**.  
- **MiniLM** surprisingly outperforms on **STS tasks** (semantic similarity).  
- **NLLB** is powerful for MT, but Turkmen performance remains low due to lack of clean training data.  

---

## ğŸ“‘ References
- Feng et al. (2020). *Language-Agnostic BERT Sentence Embedding (LaBSE).* ACL.  
- NLLB Team, Meta AI (2022). *No Language Left Behind: Scaling Human-Centered MT.* ACL.  
- Wang et al. (2020). *MiniLM: Deep Self-Attention Distillation for Pretrained Transformers.* NeurIPS.  
- Reimers & Gurevych (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.* EMNLP.  

---

## ğŸ“Œ Notes
This repository accompanies my **seminar paper (6 pages)** and **presentation slides** for the Data Science Seminar at the University of Passau.  
